\begin{abstract}
Supervised recursive neural network models (RNNs) for sentence meaning
have been successful in a wide array of sophisticated language tasks,
but it remains an open question whether they can achieve the same
results as compositional semantic grammars based in logical forms. We
address this question directly by using a logical grammar to generate
controlled data sets encoding the relationships (entailment, synonymy,
contradiction) between pairs of expressions and evaluating whether
each of two classes of neural model --- plain RNNs and recursive
neural tensor networks (RNTNs) --- can learn those relationships
correctly. Our first experiment confirms that both models can learn
the basic algebra of logical relations involved. Our second and third
experiments expand on this result to cover complex recursive
structures and sentences involving quantification. We find that the
plain RNN achieves only mixed results on the first and second
experiments, but that the stronger RNTN models generalized well in
every case. % TODO: Update to reflect results
\end{abstract}